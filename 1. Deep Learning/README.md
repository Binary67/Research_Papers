# Gradient Descent
**Gradient Descent** is one of the most popular optimization technique, widely used in MACHINE LEARNING and DEEP LEARNING. In SUPERVISED LEARNING where you have labelled data with their features, you can always use **Gradient Descent** to find the local minima or global minima of the problems.

## Analogy of Gradient Descent
You can imagine you want to go down from a hill blindfolded. You only have a gadget which tells you current height **(COST FUNCTION)** from sea level. You will start with random direction **(RANDOMLY INITIALIZED PARAMETER)** and small steps **(LEARNING RATE)**, and check the height from the gadget. Whenever the gadget shows a higher sea level, you knew that it is a wrong direction. Repeat this process and the gadget will show you the right direction **(GRADIENT)** to reach the bottom of the hill.

## Purpose of this repository
**Gradient Descent** can now be easily implemented by just a single line of code in tensorflow. However, implementation of **Gradient Descent** will help you to understand better on the math behind the algorithm.

## Further Reading
1. [Gradient Descent in Python](https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f)

2. [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)

3. [Deep Learning Paper](https://creativecoding.soe.ucsc.edu/courses/cs523/slides/week3/DeepLearning_LeCun.pdf)
